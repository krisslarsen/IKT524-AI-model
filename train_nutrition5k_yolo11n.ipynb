{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bfe3d3",
   "metadata": {},
   "source": [
    "\n",
    "# Nutrition5k √ó YOLO11 ‚Äî Classification Training (v3, Excel + aggregation support)\n",
    "\n",
    "This notebook is a **drop‚Äëin** trainer for Nutrition5k that:\n",
    "1) **Auto‚Äëdiscovers** CSV/Parquet/Feather/Pickle/**Excel** tables anywhere under your dataset root.  \n",
    "2) **Loads pickles safely** even if they were created with NumPy‚â•2 (`numpy._core`).  \n",
    "3) Uses dish‚Äëlevel totals from **`dishes.xlsx`** when present; otherwise, **aggregates** ingredient‚Äëlevel macros from `dish_ingredients.xlsx`.  \n",
    "4) **Materializes JPEGs** from `dish_images.pkl` if images are stored as bytes.  \n",
    "5) **Splits by dish**, trains **`yolo11n‚Äëcls.pt`**, and evaluates on **val** and **test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c3268ff",
   "metadata": {},
   "source": [
    "\n",
    "# --- 1) Configuration ----------------------------------------------------------\n",
    "DATASET_ROOT = \"/home/kristoffel/datasets/dataset-01-Nutrition5k\"  # ‚Üê edit if needed\n",
    "MODEL_DIR    = \"/home/kristoffel/models\"\n",
    "\n",
    "# Labeling: 'primary_macro' (carb/protein/fat) or 'calorie_bin' (5 quantiles)\n",
    "LABEL_STRATEGY = \"primary_macro\"   # ‚Üê edit me\n",
    "\n",
    "# Split ratios (per-dish)\n",
    "VAL_RATIO  = 0.10\n",
    "TEST_RATIO = 0.10\n",
    "SEED = 42\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 30\n",
    "IMGSZ  = 224\n",
    "BATCH  = 64\n",
    "RUN_NAME_BASE = \"nutrition5k_yolo11n_cls\"\n",
    "\n",
    "# Where to put materialized JPEGs if decoding from a table is needed\n",
    "RGB_DIR = f\"{DATASET_ROOT}/images_rgb\"\n",
    "\n",
    "import os, random\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "random.seed(SEED)\n",
    "\n",
    "for p in [DATASET_ROOT, MODEL_DIR]:\n",
    "    assert os.path.isdir(p), f\"Missing directory: {p}\"\n",
    "print(\"Paths OK.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfbb7c18",
   "metadata": {},
   "source": [
    "\n",
    "# --- 2) Environment (optional installs) ---------------------------------------\n",
    "# If you need dependencies for Excel/Parquet, uncomment:\n",
    "# !pip install -U ultralytics pyarrow openpyxl pandas numpy pillow\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "print(\"Ultralytics:\", ultralytics.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c13168dc",
   "metadata": {},
   "source": [
    "\n",
    "# --- 3) Discovery: find nutrition & image tables, handle Excel & pickles ------\n",
    "from pathlib import Path\n",
    "import os, pickle, pandas as pd\n",
    "\n",
    "# Custom unpickler: map 'numpy._core' -> 'numpy.core' (incl. nested paths)\n",
    "class _RemapUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == \"numpy._core\" or module.startswith(\"numpy._core.\"):\n",
    "            module = module.replace(\"numpy._core\", \"numpy.core\", 1)\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "def safe_pickle_columns(p: Path):\n",
    "    try:\n",
    "        with open(p, \"rb\") as f:\n",
    "            obj = _RemapUnpickler(f).load()\n",
    "        if hasattr(obj, \"columns\"):\n",
    "            return list(obj.columns)\n",
    "        return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def sniff_table_columns(p: Path, max_rows=5):\n",
    "    low = p.name.lower()\n",
    "    if low.endswith(\".parquet\"):\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            return [str(n) for n in pq.read_schema(p).names]\n",
    "        except Exception:\n",
    "            return []\n",
    "    if low.endswith(\".feather\"):\n",
    "        try:\n",
    "            import pyarrow.feather as ft\n",
    "            with ft.FeatherReader(str(p)) as reader:\n",
    "                return [str(reader.get_column_name(i)) for i in range(reader.num_columns)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    if low.endswith(\".csv\"):\n",
    "        try:\n",
    "            return list(pd.read_csv(p, nrows=max_rows).columns)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if low.endswith((\".pkl\",\".pickle\")):\n",
    "        return safe_pickle_columns(p)\n",
    "    if low.endswith((\".xlsx\",\".xls\")):\n",
    "        try:\n",
    "            return list(pd.read_excel(p, nrows=1).columns)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def list_candidate_files(root: str):\n",
    "    exts = (\"*.csv\",\"*.parquet\",\"*.feather\",\"*.pkl\",\"*.pickle\",\"*.xlsx\",\"*.xls\")\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(Path(root).rglob(ext))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def norm_cols(cols):\n",
    "    out = {}\n",
    "    for c in cols:\n",
    "        k = str(c).lower().strip()\n",
    "        for ch in [\" \", \"(\", \")\", \"-\", \"_\", \"/\"]:\n",
    "            k = k.replace(ch, \"\")\n",
    "        out[k] = c\n",
    "    return out\n",
    "\n",
    "# Keys to detect nutrition or image tables\n",
    "MACRO_KEYS = {\n",
    "    # calories / energy\n",
    "    \"calories\",\"kcal\",\"energykcal\",\"energy\",\"totalcalories\",\"caloriestotal\",\"calorie\",\"totalkcal\",\n",
    "    # protein\n",
    "    \"protein\",\"proteing\",\"proteingram\",\"proteingrams\",\"totalprotein\",\"proteintotal\",\n",
    "    # fat\n",
    "    \"fat\",\"fatg\",\"fatgram\",\"fatgrams\",\"totalfat\",\"fattotal\",\"lipid\",\"lipids\",\n",
    "    # carbs\n",
    "    \"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb\",\"carbsg\",\"carbgram\",\"carbgrams\",\n",
    "    \"totalcarb\",\"carbohydratetotal\",\"carbstotal\"\n",
    "}\n",
    "IMG_KEYS = {\"rgb\",\"rgbbytes\",\"image\",\"imagebytes\",\"rgbimage\",\"rgbimagebytes\",\"depth\",\"depthbytes\",\"depthimage\"}\n",
    "\n",
    "def discover_tables_robust(root: str):\n",
    "    nutr, img, ingr = None, None, None\n",
    "    cands = list_candidate_files(root)\n",
    "    print(f\"Found {len(cands)} candidate tables under {root}. Scanning schemas...\")\n",
    "    for p in cands:\n",
    "        cols = sniff_table_columns(p)\n",
    "        if not cols:\n",
    "            continue\n",
    "        nmap = norm_cols(cols)\n",
    "        keys = set(nmap.keys())\n",
    "        # ingredient file hint\n",
    "        if ingr is None and any(k.startswith(\"ingr\") for k in keys):\n",
    "            ingr = str(p)\n",
    "        # dish-level nutrition present?\n",
    "        if nutr is None and any(k in keys for k in MACRO_KEYS):\n",
    "            nutr = str(p)\n",
    "        # image table present?\n",
    "        if img is None and any(k in keys for k in IMG_KEYS):\n",
    "            img = str(p)\n",
    "        if nutr and img and ingr:\n",
    "            break\n",
    "    return nutr, img, ingr\n",
    "\n",
    "# JPEG root discovery\n",
    "JPEG_ROOT = None\n",
    "for cand in [f\"{DATASET_ROOT}/images\", f\"{DATASET_ROOT}/images_rgb\", DATASET_ROOT]:\n",
    "    if os.path.isdir(cand) and any(Path(cand).rglob(\"*.jpg\")):\n",
    "        JPEG_ROOT = cand\n",
    "        break\n",
    "\n",
    "nutr_file, img_table, ingr_table = discover_tables_robust(DATASET_ROOT)\n",
    "\n",
    "# Fallbacks\n",
    "if nutr_file is None and img_table is not None:\n",
    "    nutr_file = img_table\n",
    "    print(\"[patch] Using image table as nutrition table:\", nutr_file)\n",
    "\n",
    "# Manual filename fallbacks commonly seen in mirrors\n",
    "for guess in [\"dishes.xlsx\",\"dishes.csv\",\"dish_images.pkl\",\"dish_ingredients.xlsx\"]:\n",
    "    g = Path(DATASET_ROOT) / guess\n",
    "    if nutr_file is None and g.exists():\n",
    "        nutr_file = str(g); print(\"[manual nutr] Using:\", g)\n",
    "    if img_table is None and g.exists() and g.suffix in [\".pkl\",\".pickle\"]:\n",
    "        img_table  = str(g); print(\"[manual imgs] Using:\", g)\n",
    "    if ingr_table is None and \"ingredient\" in guess and g.exists():\n",
    "        ingr_table = str(g); print(\"[manual ingr] Using:\", g)\n",
    "\n",
    "print(\"nutrition file:\", nutr_file)\n",
    "print(\"image table   :\", img_table)\n",
    "print(\"ingredients   :\", ingr_table)\n",
    "print(\"jpeg root     :\", JPEG_ROOT)\n",
    "\n",
    "assert nutr_file or img_table, \"No usable table (nutrition or images) found under dataset root.\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install -U openpyxl",
   "id": "704cd6f5faffd015",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:21:29.861017Z",
     "start_time": "2025-12-10T10:21:25.890638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4) Load nutrition (dish-level) with fallbacks, then derive labels --------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Reuse unpickler\n",
    "class _RemapUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == \"numpy._core\" or module.startswith(\"numpy._core.\"):\n",
    "            module = module.replace(\"numpy._core\", \"numpy.core\", 1)\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "def read_pickle_compat(path: str) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = _RemapUnpickler(f).load()\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj\n",
    "    try:\n",
    "        return pd.DataFrame(obj)\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"Unsupported pickle payload type: {type(obj)} from {path}\") from e\n",
    "\n",
    "def read_any_table(path: str, usecols=None) -> pd.DataFrame:\n",
    "    low = path.lower()\n",
    "    if low.endswith(\".csv\"):\n",
    "        return pd.read_csv(path, usecols=usecols)\n",
    "    if low.endswith(\".parquet\"):\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            if usecols is None:\n",
    "                return pd.read_parquet(path)\n",
    "            table = pq.read_table(path, columns=usecols)\n",
    "            return table.to_pandas()\n",
    "        except Exception:\n",
    "            return pd.read_parquet(path, columns=usecols)\n",
    "    if low.endswith(\".feather\"):\n",
    "        try:\n",
    "            import pyarrow.feather as ft\n",
    "            tbl = ft.read_table(path)\n",
    "            if usecols:\n",
    "                tbl = tbl.select(usecols)\n",
    "            return tbl.to_pandas()\n",
    "        except Exception:\n",
    "            return pd.read_feather(path, columns=usecols)\n",
    "    if low.endswith((\".pkl\",\".pickle\")):\n",
    "        df = read_pickle_compat(path)\n",
    "        if usecols is not None:\n",
    "            keep = [c for c in usecols if c in df.columns]\n",
    "            df = df[keep]\n",
    "        return df\n",
    "    if low.endswith((\".xlsx\",\".xls\")):\n",
    "        try:\n",
    "            return pd.read_excel(path, usecols=usecols)\n",
    "        except Exception:\n",
    "            # try engine fallback\n",
    "            return pd.read_excel(path, usecols=usecols, engine=\"openpyxl\")\n",
    "    raise ValueError(f\"Unsupported table format: {path}\")\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    for ch in [\" \", \"(\", \")\", \"-\", \"_\", \"/\"]:\n",
    "        s = s.replace(ch, \"\")\n",
    "    return s\n",
    "\n",
    "def pick_from(df_cols, *alts):\n",
    "    nmap = {_norm(c): c for c in df_cols}\n",
    "    for a in alts:\n",
    "        if _norm(a) in nmap:\n",
    "            return nmap[_norm(a)]\n",
    "    return None\n",
    "\n",
    "# Load a small sample first to detect columns\n",
    "probe = read_any_table(nutr_file)\n",
    "cols = list(probe.columns)\n",
    "print(\"Detected columns in nutr_file (first 25):\", cols[:25])\n",
    "\n",
    "# ‚îÄ‚îÄ Try dish id and totals (FIXED: add cal_col; unify carb_col) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dish_col = pick_from(cols, \"dish_id\",\"plate_id\",\"sample_id\",\"id\",\"dish\",\"plateid\",\"dishid\")\n",
    "cal_col  = pick_from(cols, \"calories\",\"kcal\",\"energy_kcal\",\"energykcal\",\"energy\",\n",
    "                     \"total_calories\",\"calories_total\",\"total_kcal\",\"calorie\",\"totalcalories\")  # ‚Üê added back\n",
    "pro_col  = pick_from(cols, \"protein\",\"protein_g\",\"proteing\",\"total_protein\",\"protein_total\",\"totalprotein\")\n",
    "fat_col  = pick_from(cols, \"fat\",\"fat_g\",\"fatg\",\"total_fat\",\"fat_total\",\"totalfat\")\n",
    "carb_col = pick_from(cols, \"carb\",\"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb_g\",\n",
    "                     \"carbohydrates_g\",\"carbohydrateg\",\"total_carb\",\"carbs_total\",\"totalcarb\")  # ‚Üê single line\n",
    "\n",
    "print(\"Dish/Nutrition columns ->\",\n",
    "      \"dish:\", dish_col, \"| kcal:\", cal_col, \"| protein:\", pro_col, \"| fat:\", fat_col, \"| carbs:\", carb_col)\n",
    "\n",
    "# If no totals present, but we have an ingredient table, aggregate it\n",
    "if (cal_col is None or pro_col is None or fat_col is None or carb_col is None) and 'ingr_table' in globals() and ingr_table:\n",
    "    print(\"[aggregate] Dish totals not found in nutr_file; attempting aggregation from ingredient table:\", ingr_table)\n",
    "    df_ing = read_any_table(ingr_table)\n",
    "    icolumns = list(df_ing.columns)\n",
    "    print(\"Ingredient columns (first 25):\", icolumns[:25])\n",
    "\n",
    "    di_col = pick_from(icolumns, \"dish_id\",\"plate_id\",\"sample_id\",\"id\",\"dish\",\"plateid\",\"dishid\")\n",
    "    ical   = pick_from(icolumns, \"calories\",\"kcal\",\"energy_kcal\",\"energykcal\",\"energy\",\n",
    "                       \"ingr_calories\",\"ingrcalories\")\n",
    "    ipro   = pick_from(icolumns, \"protein\",\"protein_g\",\"proteing\",\"ingr_protein\",\"ingrprotein\")\n",
    "    ifat   = pick_from(icolumns, \"fat\",\"fat_g\",\"fatg\",\"totalfat\",\"ingr_fat\",\"ingrfat\")\n",
    "    icrb   = pick_from(icolumns, \"carb\",\"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb_g\",\n",
    "                       \"ingr_carb\",\"ingr_carbohydrate\",\"ingrcarb\")\n",
    "    assert di_col is not None, \"Ingredient table lacks a dish id column.\"\n",
    "\n",
    "    # Coerce numeric and aggregate (robust: skip missing)\n",
    "    for c in [ical, ipro, ifat, icrb]:\n",
    "        if c is not None:\n",
    "            df_ing[c] = pd.to_numeric(df_ing[c], errors=\"coerce\")\n",
    "\n",
    "    agg_spec = {}\n",
    "    if ical: agg_spec[ical] = \"sum\"\n",
    "    if ipro: agg_spec[ipro] = \"sum\"\n",
    "    if ifat: agg_spec[ifat] = \"sum\"\n",
    "    if icrb: agg_spec[icrb] = \"sum\"\n",
    "    if not agg_spec:\n",
    "        raise RuntimeError(\"No numeric ingredient columns to aggregate.\")\n",
    "\n",
    "    df_tot = df_ing.groupby(di_col, as_index=False).agg(agg_spec)\n",
    "\n",
    "    rename_map = {}\n",
    "    if ical: rename_map[ical] = \"total_calories\"\n",
    "    if ipro: rename_map[ipro] = \"total_protein\"\n",
    "    if ifat: rename_map[ifat] = \"total_fat\"\n",
    "    if icrb: rename_map[icrb] = \"total_carb\"\n",
    "    df_tot = df_tot.rename(columns=rename_map)\n",
    "\n",
    "    if di_col != \"dish_id\":\n",
    "        df_tot = df_tot.rename(columns={di_col: \"dish_id\"})\n",
    "\n",
    "    # Update pointers only for fields we actually built\n",
    "    dish_col = \"dish_id\"\n",
    "    if ical: cal_col  = \"total_calories\"\n",
    "    if ipro: pro_col  = \"total_protein\"\n",
    "    if ifat: fat_col  = \"total_fat\"\n",
    "    if icrb: carb_col = \"total_carb\"\n",
    "\n",
    "    df_nutr = df_tot.copy()\n",
    "else:\n",
    "    # Use nutr_file as-is\n",
    "    df_nutr = probe.copy()\n",
    "\n",
    "# Ensure required columns exist\n",
    "if dish_col is None:\n",
    "    raise RuntimeError(\"Could not find a 'dish id' column. Please inspect the printed columns.\")\n",
    "need_any = [cal_col, pro_col, fat_col, carb_col]\n",
    "if not any(need_any):\n",
    "    raise RuntimeError(\"Neither calories nor macro columns were found. Please verify your dataset files (expecting dishes.xlsx or dish_ingredients.xlsx).\")\n",
    "\n",
    "# Build labels\n",
    "df_nutr = df_nutr.dropna(subset=[dish_col]).copy()\n",
    "\n",
    "if LABEL_STRATEGY == \"primary_macro\" and all(c is not None for c in [pro_col, fat_col, carb_col]):\n",
    "    def _tofloat(x):\n",
    "        try: return float(x)\n",
    "        except Exception: return np.nan\n",
    "    def primary_macro(r):\n",
    "        p = _tofloat(r[pro_col]); f = _tofloat(r[fat_col]); c = _tofloat(r[carb_col])\n",
    "        p = 0.0 if np.isnan(p) else p\n",
    "        f = 0.0 if np.isnan(f) else f\n",
    "        c = 0.0 if np.isnan(c) else c\n",
    "        return max([(p,\"protein\"), (f,\"fat\"), (c,\"carb\")])[1]\n",
    "    df_nutr[\"label\"] = df_nutr.apply(primary_macro, axis=1)\n",
    "    class_names = [\"carb\",\"protein\",\"fat\"]\n",
    "\n",
    "elif LABEL_STRATEGY == \"calorie_bin\" and cal_col is not None:\n",
    "    df_nutr = df_nutr.dropna(subset=[cal_col])\n",
    "    df_nutr[\"label\"] = pd.qcut(pd.to_numeric(df_nutr[cal_col], errors=\"coerce\"),\n",
    "                               q=5, duplicates=\"drop\").astype(str)\n",
    "    class_names = sorted(df_nutr[\"label\"].dropna().unique().tolist())\n",
    "\n",
    "else:\n",
    "    # Fallback to calorie bins if macros missing but calories exist\n",
    "    if cal_col is not None:\n",
    "        df_nutr = df_nutr.dropna(subset=[cal_col])\n",
    "        df_nutr[\"label\"] = pd.qcut(pd.to_numeric(df_nutr[cal_col], errors=\"coerce\"),\n",
    "                                   q=5, duplicates=\"drop\").astype(str)\n",
    "        LABEL_STRATEGY = \"calorie_bin\"\n",
    "        class_names = sorted(df_nutr[\"label\"].dropna().unique().tolist())\n",
    "        print(\"Fallback: using LABEL_STRATEGY='calorie_bin'\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No usable columns for labeling.\")\n",
    "\n",
    "# Final label table\n",
    "df_lbl = (df_nutr[[dish_col, \"label\"]]\n",
    "          .dropna()\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={dish_col: \"dish_id\"})\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "print(\"Label distribution (top 10):\")\n",
    "print(df_lbl[\"label\"].value_counts().head(10))\n",
    "\n",
    "# Export core vars\n",
    "globals().update({\n",
    "    \"dish_col\": dish_col, \"cal_col\": cal_col, \"pro_col\": pro_col,\n",
    "    \"fat_col\": fat_col, \"carb_col\": carb_col, \"df_lbl\": df_lbl,\n",
    "    \"LABEL_STRATEGY\": LABEL_STRATEGY\n",
    "})\n"
   ],
   "id": "a4b75cca6663c567",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns in nutr_file (first 25): ['dish_id', 'ingr_id', 'ingr_name', 'grams', 'calories', 'fat', 'carb', 'protein']\n",
      "Dish/Nutrition columns -> dish: dish_id | kcal: calories | protein: protein | fat: fat | carbs: carb\n",
      "Label distribution (top 10):\n",
      "carb       4380\n",
      "fat        2818\n",
      "protein    2379\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "cf75da60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:22:05.095941Z",
     "start_time": "2025-12-10T10:21:58.299904Z"
    }
   },
   "source": [
    "\n",
    "# --- 4) Load nutrition (dish-level) with fallbacks, then derive labels --------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Reuse unpickler\n",
    "class _RemapUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == \"numpy._core\" or module.startswith(\"numpy._core.\"):\n",
    "            module = module.replace(\"numpy._core\", \"numpy.core\", 1)\n",
    "        return super().find_class(module, name)\n",
    "\n",
    "def read_pickle_compat(path: str) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = _RemapUnpickler(f).load()\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj\n",
    "    try:\n",
    "        return pd.DataFrame(obj)\n",
    "    except Exception as e:\n",
    "        raise TypeError(f\"Unsupported pickle payload type: {type(obj)} from {path}\") from e\n",
    "\n",
    "def read_any_table(path: str, usecols=None) -> pd.DataFrame:\n",
    "    low = path.lower()\n",
    "    if low.endswith(\".csv\"):\n",
    "        return pd.read_csv(path, usecols=usecols)\n",
    "    if low.endswith(\".parquet\"):\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            if usecols is None:\n",
    "                return pd.read_parquet(path)\n",
    "            table = pq.read_table(path, columns=usecols)\n",
    "            return table.to_pandas()\n",
    "        except Exception:\n",
    "            return pd.read_parquet(path, columns=usecols)\n",
    "    if low.endswith(\".feather\"):\n",
    "        try:\n",
    "            import pyarrow.feather as ft\n",
    "            tbl = ft.read_table(path)\n",
    "            if usecols:\n",
    "                tbl = tbl.select(usecols)\n",
    "            return tbl.to_pandas()\n",
    "        except Exception:\n",
    "            return pd.read_feather(path, columns=usecols)\n",
    "    if low.endswith((\".pkl\",\".pickle\")):\n",
    "        df = read_pickle_compat(path)\n",
    "        if usecols is not None:\n",
    "            keep = [c for c in usecols if c in df.columns]\n",
    "            df = df[keep]\n",
    "        return df\n",
    "    if low.endswith((\".xlsx\",\".xls\")):\n",
    "        try:\n",
    "            return pd.read_excel(path, usecols=usecols)\n",
    "        except Exception:\n",
    "            # try engine fallback\n",
    "            return pd.read_excel(path, usecols=usecols, engine=\"openpyxl\")\n",
    "    raise ValueError(f\"Unsupported table format: {path}\")\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = str(s).lower().strip()\n",
    "    for ch in [\" \", \"(\", \")\", \"-\", \"_\", \"/\"]:\n",
    "        s = s.replace(ch, \"\")\n",
    "    return s\n",
    "\n",
    "def pick_from(df_cols, *alts):\n",
    "    nmap = {_norm(c): c for c in df_cols}\n",
    "    for a in alts:\n",
    "        if _norm(a) in nmap:\n",
    "            return nmap[_norm(a)]\n",
    "    return None\n",
    "\n",
    "# Load a small sample first to detect columns\n",
    "probe = read_any_table(nutr_file)\n",
    "cols = list(probe.columns)\n",
    "print(\"Detected columns in nutr_file (first 25):\", cols[:25])\n",
    "\n",
    "# Try dish id and totals\n",
    "dish_col = pick_from(cols, \"dish_id\",\"plate_id\",\"sample_id\",\"id\",\"dish\",\"plateid\",\"dishid\")\n",
    "carb_col = pick_from(cols, \"carb\",\"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb_g\",\"carbohydrates_g\",\"carbohydrateg\")\n",
    "pro_col  = pick_from(cols, \"protein\",\"protein_g\",\"proteing\",\"total_protein\",\"protein_total\",\"totalprotein\")\n",
    "fat_col  = pick_from(cols, \"fat\",\"fat_g\",\"fatg\",\"total_fat\",\"fat_total\",\"totalfat\")\n",
    "carb_col = pick_from(cols, \"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb_g\",\"carbohydrates_g\",\"carbohydrateg\",\n",
    "                     \"total_carb\",\"carbs_total\",\"totalcarb\")\n",
    "\n",
    "print(\"Dish/Nutrition columns ->\",\n",
    "      \"dish:\", dish_col, \"| kcal:\", cal_col, \"| protein:\", pro_col, \"| fat:\", fat_col, \"| carbs:\", carb_col)\n",
    "\n",
    "# If no totals present, but we have an ingredient table, aggregate it\n",
    "if (cal_col is None or pro_col is None or fat_col is None or carb_col is None) and 'ingr_table' in globals() and ingr_table:\n",
    "    print(\"[aggregate] Dish totals not found in nutr_file; attempting aggregation from ingredient table:\", ingr_table)\n",
    "    df_ing = read_any_table(ingr_table)\n",
    "    icolumns = list(df_ing.columns)\n",
    "    print(\"Ingredient columns (first 25):\", icolumns[:25])\n",
    "    di_col = pick_from(icolumns, \"dish_id\",\"plate_id\",\"sample_id\",\"id\",\"dish\",\"plateid\",\"dishid\")\n",
    "    ical  = pick_from(icolumns, \"ingr_calories\",\"calories\",\"kcal\",\"energy_kcal\",\"energykcal\",\"ingrcalories\")\n",
    "    ipro  = pick_from(icolumns, \"ingr_protein\",\"protein\",\"protein_g\",\"proteing\",\"ingrprotein\")\n",
    "    ifat  = pick_from(icolumns, \"ingr_fat\",\"fat\",\"fat_g\",\"fatg\",\"ingrfat\")\n",
    "    icrb  = pick_from(icolumns, \"carb\",\"ingr_carb\",\"ingr_carbohydrate\",\"carbohydrate\",\"carbohydrates\",\"carbs\",\"carb_g\",\"ingrcarb\")\n",
    "    assert di_col is not None, \"Ingredient table lacks a dish id column.\"\n",
    "    # Coerce numeric and aggregate\n",
    "    for c in [ical, ipro, ifat, icrb]:\n",
    "        if c is not None:\n",
    "            df_ing[c] = pd.to_numeric(df_ing[c], errors=\"coerce\")\n",
    "    agg_spec = {}\n",
    "    if ical: agg_spec[ical] = \"sum\"\n",
    "    if ipro: agg_spec[ipro] = \"sum\"\n",
    "    if ifat: agg_spec[ifat] = \"sum\"\n",
    "    if icrb: agg_spec[icrb] = \"sum\"\n",
    "    if not agg_spec:\n",
    "        raise RuntimeError(\"No numeric ingredient columns to aggregate.\")\n",
    "\n",
    "    df_tot = df_ing.groupby(di_col, as_index=False).agg(agg_spec)\n",
    "\n",
    "    rename_map = {}\n",
    "    if ical: rename_map[ical] = \"total_calories\"\n",
    "    if ipro: rename_map[ipro] = \"total_protein\"\n",
    "    if ifat: rename_map[ifat] = \"total_fat\"\n",
    "    if icrb: rename_map[icrb] = \"total_carb\"\n",
    "\n",
    "    df_tot = df_tot.rename(columns=rename_map)\n",
    "    if di_col != \"dish_id\":\n",
    "        df_tot = df_tot.rename(columns={di_col: \"dish_id\"})\n",
    "    dish_col = \"dish_id\"; cal_col = \"total_calories\"; pro_col = \"total_protein\"; fat_col = \"total_fat\"; carb_col = \"total_carb\"\n",
    "    df_nutr = df_tot.copy()\n",
    "else:\n",
    "    # Use nutr_file as-is\n",
    "    df_nutr = probe.copy()\n",
    "\n",
    "# Ensure required columns exist\n",
    "if dish_col is None:\n",
    "    raise RuntimeError(\"Could not find a 'dish id' column. Please inspect the printed columns.\")\n",
    "need_any = [cal_col, pro_col, fat_col, carb_col]\n",
    "if not any(need_any):\n",
    "    raise RuntimeError(\"Neither calories nor macro columns were found. Please verify your dataset files (expecting dishes.xlsx or dish_ingredients.xlsx).\")\n",
    "\n",
    "# Build labels\n",
    "df_nutr = df_nutr.dropna(subset=[dish_col]).copy()\n",
    "\n",
    "if LABEL_STRATEGY == \"primary_macro\" and all(c is not None for c in [pro_col, fat_col, carb_col]):\n",
    "    def _tofloat(x):\n",
    "        try: return float(x)\n",
    "        except Exception: return np.nan\n",
    "    def primary_macro(r):\n",
    "        p = _tofloat(r[pro_col]); f = _tofloat(r[fat_col]); c = _tofloat(r[carb_col])\n",
    "        p = 0.0 if np.isnan(p) else p\n",
    "        f = 0.0 if np.isnan(f) else f\n",
    "        c = 0.0 if np.isnan(c) else c\n",
    "        return max([(p,\"protein\"), (f,\"fat\"), (c,\"carb\")])[1]\n",
    "    df_nutr[\"label\"] = df_nutr.apply(primary_macro, axis=1)\n",
    "    class_names = [\"carb\",\"protein\",\"fat\"]\n",
    "\n",
    "elif LABEL_STRATEGY == \"calorie_bin\" and cal_col is not None:\n",
    "    df_nutr = df_nutr.dropna(subset=[cal_col])\n",
    "    df_nutr[\"label\"] = pd.qcut(pd.to_numeric(df_nutr[cal_col], errors=\"coerce\"),\n",
    "                               q=5, duplicates=\"drop\").astype(str)\n",
    "    class_names = sorted(df_nutr[\"label\"].dropna().unique().tolist())\n",
    "\n",
    "else:\n",
    "    # Fallback to calorie bins if macros missing but calories exist\n",
    "    if cal_col is not None:\n",
    "        df_nutr = df_nutr.dropna(subset=[cal_col])\n",
    "        df_nutr[\"label\"] = pd.qcut(pd.to_numeric(df_nutr[cal_col], errors=\"coerce\"),\n",
    "                                   q=5, duplicates=\"drop\").astype(str)\n",
    "        LABEL_STRATEGY = \"calorie_bin\"\n",
    "        class_names = sorted(df_nutr[\"label\"].dropna().unique().tolist())\n",
    "        print(\"Fallback: using LABEL_STRATEGY='calorie_bin'\")\n",
    "    else:\n",
    "        raise RuntimeError(\"No usable columns for labeling.\")\n",
    "\n",
    "# Final label table\n",
    "df_lbl = (df_nutr[[dish_col, \"label\"]]\n",
    "          .dropna()\n",
    "          .drop_duplicates()\n",
    "          .rename(columns={dish_col: \"dish_id\"})\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "print(\"Label distribution (top 10):\")\n",
    "print(df_lbl[\"label\"].value_counts().head(10))\n",
    "\n",
    "# Export core vars\n",
    "globals().update({\n",
    "    \"dish_col\": dish_col, \"cal_col\": cal_col, \"pro_col\": pro_col,\n",
    "    \"fat_col\": fat_col, \"carb_col\": carb_col, \"df_lbl\": df_lbl,\n",
    "    \"LABEL_STRATEGY\": LABEL_STRATEGY\n",
    "})\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected columns in nutr_file (first 25): ['dish_id', 'ingr_id', 'ingr_name', 'grams', 'calories', 'fat', 'carb', 'protein']\n",
      "Dish/Nutrition columns -> dish: dish_id | kcal: calories | protein: protein | fat: fat | carbs: None\n",
      "[aggregate] Dish totals not found in nutr_file; attempting aggregation from ingredient table: /home/kristoffel/datasets/dataset-01-Nutrition5k/dish_ingredients.xlsx\n",
      "Ingredient columns (first 25): ['dish_id', 'ingr_id', 'ingr_name', 'grams', 'calories', 'fat', 'carb', 'protein']\n",
      "Label distribution (top 10):\n",
      "carb       2907\n",
      "protein    1244\n",
      "fat         855\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "15e65ad4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:22:51.942115Z",
     "start_time": "2025-12-10T10:22:10.822172Z"
    }
   },
   "source": [
    "\n",
    "# --- 5) Materialize JPEGs if needed -------------------------------------------\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import base64, ast\n",
    "\n",
    "# Prefer ready-made JPEGs\n",
    "JPEG_ROOT = globals().get(\"JPEG_ROOT\", None)\n",
    "if JPEG_ROOT is None:\n",
    "    # If we have an image table, infer columns and decode\n",
    "    img_table = globals().get(\"img_table\", None)\n",
    "    assert img_table, \"No JPEGs found and no image table located. Provide an images table or mirror with JPEGs.\"\n",
    "    # Detect columns\n",
    "    df_img = read_any_table(img_table)\n",
    "    cnorm = {str(c).lower().strip().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"_\",\"\"): c\n",
    "             for c in df_img.columns}\n",
    "    dish_img_col = None\n",
    "    for a in [\"dish_id\",\"plate_id\",\"sample_id\",\"id\",\"dish\",\"plateid\",\"dishid\"]:\n",
    "        k = a.lower().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"_\",\"\")\n",
    "        if k in cnorm:\n",
    "            dish_img_col = cnorm[k]; break\n",
    "    assert dish_img_col is not None, \"Could not find a dish id column in images table.\"\n",
    "\n",
    "    rgb_col = None\n",
    "    for a in [\"rgb\",\"rgb_bytes\",\"image\",\"image_bytes\",\"rgbimage\",\"rgbimagebytes\",\"rgb_image\"]:\n",
    "        k = a.lower().replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"_\",\"\")\n",
    "        if k in cnorm:\n",
    "            rgb_col = cnorm[k]; break\n",
    "    assert rgb_col is not None, \"Could not find an RGB image bytes column.\"\n",
    "\n",
    "    out_root = Path(RGB_DIR); out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def decode_to_bytes(x):\n",
    "        if isinstance(x, (bytes, bytearray)):\n",
    "            return bytes(x)\n",
    "        if isinstance(x, memoryview):\n",
    "            return x.tobytes()\n",
    "        if isinstance(x, str):\n",
    "            s = x.strip()\n",
    "            if (s.startswith(\"b'\") or s.startswith('b\"')) and s.endswith((\"'\",'\"')):\n",
    "                try: return ast.literal_eval(s)\n",
    "                except Exception: pass\n",
    "            try:\n",
    "                return base64.b64decode(s, validate=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            return bytes(x)\n",
    "        except Exception:\n",
    "            raise ValueError(\"Unsupported RGB encoding type\")\n",
    "\n",
    "    count, skipped = 0, 0\n",
    "    for ridx, row in df_img.iterrows():\n",
    "        dish_id = str(row[dish_img_col])\n",
    "        try:\n",
    "            rgb_bytes = decode_to_bytes(row[rgb_col])\n",
    "            img = Image.open(BytesIO(rgb_bytes)).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        dish_dir = out_root / dish_id\n",
    "        dish_dir.mkdir(exist_ok=True, parents=True)\n",
    "        out_path = dish_dir / f\"img_{len(list(dish_dir.glob('img_*.jpg'))):05d}.jpg\"\n",
    "        img.save(out_path, format=\"JPEG\", quality=90)\n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print(\"saved\", count, \"images...\")\n",
    "    print(f\"Saved {count} images to {out_root} ({skipped} skipped).\")\n",
    "    JPEG_ROOT = str(out_root)\n",
    "else:\n",
    "    print(\"Found JPEGs under:\", JPEG_ROOT)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 500 images...\n",
      "saved 1000 images...\n",
      "saved 1500 images...\n",
      "saved 2000 images...\n",
      "saved 2500 images...\n",
      "saved 3000 images...\n",
      "Saved 3490 images to /home/kristoffel/datasets/dataset-01-Nutrition5k/images_rgb (0 skipped).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "a87ce7a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:23:12.346051Z",
     "start_time": "2025-12-10T10:23:11.991932Z"
    }
   },
   "source": [
    "\n",
    "# --- 6) Build train/val/test splits by dish -----------------------------------\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Map dish_id -> list of image paths\n",
    "dish_to_paths = defaultdict(list)\n",
    "for p in Path(JPEG_ROOT).rglob(\"*.jpg\"):\n",
    "    dish_id = p.parent.name\n",
    "    dish_to_paths[dish_id].append(str(p))\n",
    "\n",
    "label_map = dict(df_lbl.values)  # dish_id -> label\n",
    "pairs = [(d, label_map.get(d, None), imgs) for d, imgs in dish_to_paths.items()]\n",
    "pairs = [(d,l,imgs) for (d,l,imgs) in pairs if l is not None and len(imgs)>0]\n",
    "\n",
    "print(\"Dishes with labels & images:\", len(pairs))\n",
    "by_label = defaultdict(list)\n",
    "for dish, label, imgs in pairs:\n",
    "    by_label[label].append((dish, imgs))\n",
    "\n",
    "TRAIN_DIR = f\"{DATASET_ROOT}/train\"\n",
    "VAL_DIR   = f\"{DATASET_ROOT}/val\"\n",
    "TEST_DIR  = f\"{DATASET_ROOT}/test\"\n",
    "\n",
    "# Recreate split directories\n",
    "for d in (TRAIN_DIR, VAL_DIR, TEST_DIR):\n",
    "    if os.path.isdir(d):\n",
    "        shutil.rmtree(d)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    for cls in sorted(set(df_lbl['label'])):\n",
    "        os.makedirs(os.path.join(d, cls), exist_ok=True)\n",
    "\n",
    "def safe_link(src, dst):\n",
    "    try:\n",
    "        os.symlink(src, dst)\n",
    "    except Exception:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "splits = {\"train\": [], \"val\": [], \"test\": []}\n",
    "for label, items in by_label.items():\n",
    "    random.shuffle(items)\n",
    "    n = len(items)\n",
    "    n_val  = max(1, int(n * VAL_RATIO))\n",
    "    n_test = max(1, int(n * TEST_RATIO))\n",
    "    splits[\"val\"].extend([(label, d, imgs) for d,imgs in items[:n_val]])\n",
    "    splits[\"test\"].extend([(label, d, imgs) for d,imgs in items[n_val:n_val+n_test]])\n",
    "    splits[\"train\"].extend([(label, d, imgs) for d,imgs in items[n_val+n_test:]])\n",
    "\n",
    "def materialize(split_name):\n",
    "    cnt = 0\n",
    "    for label, dish, imgs in splits[split_name]:\n",
    "        for src in imgs:\n",
    "            dst = os.path.join(DATASET_ROOT, split_name, label, f\"{dish}__{Path(src).name}\")\n",
    "            if not os.path.exists(dst):\n",
    "                safe_link(src, dst)\n",
    "                cnt += 1\n",
    "    print(f\"{split_name}: wrote {cnt} image links/files.\")\n",
    "\n",
    "materialize(\"train\"); materialize(\"val\"); materialize(\"test\")\n",
    "\n",
    "# Clear stale caches for Ultralytics\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    cache = Path(DATASET_ROOT)/f\"{split}.cache\"\n",
    "    if cache.exists():\n",
    "        cache.unlink()\n",
    "        print(\"removed cache:\", cache)\n",
    "\n",
    "print(\"‚úÖ Splits ready.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dishes with labels & images: 3490\n",
      "train: wrote 2794 image links/files.\n",
      "val: wrote 348 image links/files.\n",
      "test: wrote 348 image links/files.\n",
      "‚úÖ Splits ready.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "bd38631f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:24:52.367222Z",
     "start_time": "2025-12-10T10:23:16.443833Z"
    }
   },
   "source": [
    "\n",
    "# --- 7) Train YOLO11n-cls -----------------------------------------------------\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model = YOLO(\"yolo11n-cls.pt\")\n",
    "results = model.train(\n",
    "    data=DATASET_ROOT,     # directory with train/val/test\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    lr0=1e-3,\n",
    "    patience=10,\n",
    "    project=MODEL_DIR,\n",
    "    name=RUN_NAME_BASE + (\"_macro\" if LABEL_STRATEGY==\"primary_macro\" else \"_calbins\"),\n",
    "    plots=True,\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "print(\"Training run saved to:\", results.save_dir)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.235 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.203 üöÄ Python-3.10.12 torch-2.2.0a0+81ea7a4 CUDA:0 (Tesla V100-SXM3-32GB, 32494MiB)\n",
      "\u001B[34m\u001B[1mengine/trainer: \u001B[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/kristoffel/datasets/dataset-01-Nutrition5k, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=nutrition5k_yolo11n_cls_macro, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/home/kristoffel/models, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/kristoffel/models/nutrition5k_yolo11n_cls_macro, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001B[34m\u001B[1mtrain:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/train... found 2794 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mval:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/val... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mtest:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/test... found 348 images in 3 classes ‚úÖ \n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 10                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
      "YOLO11n-cls summary: 86 layers, 1,534,947 parameters, 1,534,947 gradients, 3.3 GFLOPs\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed ‚úÖ\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1352.6¬±331.9 MB/s, size: 52.9 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mtrain: \u001B[0mScanning /home/kristoffel/datasets/dataset-01-Nutrition5k/train... 2794 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2794/2794 5.9Kit/s 0.5s0.1s\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mNew cache created: /home/kristoffel/datasets/dataset-01-Nutrition5k/train.cache\n",
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 916.5¬±518.5 MB/s, size: 39.5 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mval: \u001B[0mScanning /home/kristoffel/datasets/dataset-01-Nutrition5k/val... 348 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 348/348 5.5Kit/s 0.1s\n",
      "\u001B[34m\u001B[1mval: \u001B[0mNew cache created: /home/kristoffel/datasets/dataset-01-Nutrition5k/val.cache\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001B[1m/home/kristoffel/models/nutrition5k_yolo11n_cls_macro\u001B[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       1/30     0.924G     0.9266         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 6.7it/s 6.6s<0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 10.1it/s 0.3s.0s\n",
      "                   all      0.687          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       2/30     0.996G     0.6743         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 20.5it/s 2.1s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 43.5it/s 0.1s\n",
      "                   all      0.695          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       3/30     0.996G     0.6903         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 25.0it/s 1.8s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 60.9it/s 0.0s\n",
      "                   all      0.655          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       4/30     0.996G     0.6958         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 20.0it/s 2.2s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.5it/s 0.0s\n",
      "                   all      0.655          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       5/30     0.996G     0.6163         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.0it/s 2.4s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.8it/s 0.0s\n",
      "                   all      0.718          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       6/30     0.996G     0.6028         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.1it/s 2.4s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.1it/s 0.0s\n",
      "                   all      0.701          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       7/30     0.996G     0.5786         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.0it/s 2.4s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 51.0it/s 0.1s\n",
      "                   all      0.744          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       8/30     0.996G     0.5718         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.5it/s 2.4s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.1it/s 0.0s\n",
      "                   all       0.73          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K       9/30     0.996G     0.5296         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 19.3it/s 2.3s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 57.0it/s 0.1s\n",
      "                   all       0.77          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      10/30     0.996G     0.5188         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.2it/s 2.4s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.0it/s 0.0s\n",
      "                   all      0.776          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      11/30     0.996G     0.4713         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 19.3it/s 2.3s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 59.6it/s 0.1s\n",
      "                   all      0.687          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      12/30     0.996G     0.5078         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.1it/s 2.4s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 58.3it/s 0.1s\n",
      "                   all      0.761          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      13/30     0.996G     0.4636         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 19.5it/s 2.3s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 54.1it/s 0.1s\n",
      "                   all      0.741          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      14/30     0.996G      0.432         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.1it/s 2.4s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 54.2it/s 0.1s\n",
      "                   all      0.747          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      15/30     0.996G     0.4328         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 16.6it/s 2.6s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.5it/s 0.0s\n",
      "                   all      0.784          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      16/30     0.996G     0.3954         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.8it/s 2.5s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.4it/s 0.0s\n",
      "                   all      0.802          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      17/30     0.996G     0.3924         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 16.5it/s 2.7s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.9it/s 0.0s\n",
      "                   all      0.764          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      18/30     0.996G      0.386         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.4it/s 2.5s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.4it/s 0.0s\n",
      "                   all      0.825          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      19/30     0.996G     0.3442         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 16.3it/s 2.7s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.2it/s 0.0s\n",
      "                   all      0.813          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      20/30     0.996G     0.3451         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.2it/s 2.6s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 62.2it/s 0.0s\n",
      "                   all      0.819          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      21/30     0.996G      0.317         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 16.3it/s 2.7s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 59.2it/s 0.1s\n",
      "                   all      0.842          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      22/30     0.996G     0.2914         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.0it/s 2.4s0.1s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.4it/s 0.0s\n",
      "                   all       0.81          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      23/30     0.996G     0.2849         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.3it/s 2.5s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.1it/s 0.0s\n",
      "                   all      0.807          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      24/30     0.996G     0.2627         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.3it/s 2.4s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.0it/s 0.0s\n",
      "                   all      0.833          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      25/30     0.996G     0.2499         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 16.7it/s 2.6s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.8it/s 0.0s\n",
      "                   all      0.833          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      26/30     0.996G     0.2351         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.0it/s 2.4s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 50.9it/s 0.1s\n",
      "                   all      0.856          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      27/30     0.996G     0.2233         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.2it/s 2.6s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.4it/s 0.0s\n",
      "                   all      0.839          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      28/30     0.996G     0.2028         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.4it/s 2.4s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.1it/s 0.0s\n",
      "                   all       0.83          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      29/30     0.996G     0.1943         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 17.5it/s 2.5s0.2s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 61.3it/s 0.0s\n",
      "                   all      0.819          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001B[K      30/30     0.996G     0.1792         42        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44/44 18.3it/s 2.4s0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 60.3it/s 0.0s\n",
      "                   all      0.836          1\n",
      "\n",
      "30 epochs completed in 0.024 hours.\n",
      "Optimizer stripped from /home/kristoffel/models/nutrition5k_yolo11n_cls_macro/weights/last.pt, 3.2MB\n",
      "Optimizer stripped from /home/kristoffel/models/nutrition5k_yolo11n_cls_macro/weights/best.pt, 3.2MB\n",
      "\n",
      "Validating /home/kristoffel/models/nutrition5k_yolo11n_cls_macro/weights/best.pt...\n",
      "Ultralytics 8.3.203 üöÄ Python-3.10.12 torch-2.2.0a0+81ea7a4 CUDA:0 (Tesla V100-SXM3-32GB, 32494MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001B[34m\u001B[1mtrain:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/train... found 2794 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mval:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/val... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mtest:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/test... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 32.5it/s 0.1s\n",
      "                   all      0.856          1\n",
      "Speed: 0.1ms preprocess, 0.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001B[1m/home/kristoffel/models/nutrition5k_yolo11n_cls_macro\u001B[0m\n",
      "Training run saved to: /home/kristoffel/models/nutrition5k_yolo11n_cls_macro\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6689b908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T10:39:44.347444Z",
     "start_time": "2025-12-10T10:39:38.176888Z"
    }
   },
   "source": [
    "\n",
    "# --- 8) Evaluate on val and test ----------------------------------------------\n",
    "from ultralytics import YOLO\n",
    "import glob, os, torch\n",
    "\n",
    "RUN_PREFIX = RUN_NAME_BASE\n",
    "cands = glob.glob(os.path.join(MODEL_DIR, RUN_PREFIX + \"*\", \"weights\", \"best.pt\"))\n",
    "assert cands, f\"No best.pt found under {MODEL_DIR}/{RUN_PREFIX}*/weights/\"\n",
    "best_path = max(cands, key=os.path.getmtime)\n",
    "print(\"Using best:\", best_path)\n",
    "\n",
    "model = YOLO(best_path)\n",
    "\n",
    "metrics_val = model.val(\n",
    "    data=DATASET_ROOT,\n",
    "    split=\"val\",\n",
    "    imgsz=IMGSZ,\n",
    "    project=MODEL_DIR,\n",
    "    name=RUN_PREFIX + \"_val\",\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "metrics_test = model.val(\n",
    "    data=DATASET_ROOT,\n",
    "    split=\"test\",\n",
    "    imgsz=IMGSZ,\n",
    "    project=MODEL_DIR,\n",
    "    name=RUN_PREFIX + \"_test\",\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "print(\"Done.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best: /home/kristoffel/models/nutrition5k_yolo11n_cls_macro/weights/best.pt\n",
      "Ultralytics 8.3.203 üöÄ Python-3.10.12 torch-2.2.0a0+81ea7a4 CUDA:0 (Tesla V100-SXM3-32GB, 32494MiB)\n",
      "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
      "\u001B[34m\u001B[1mtrain:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/train... found 2794 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mval:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/val... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mtest:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/test... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mval: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1136.5¬±320.5 MB/s, size: 39.5 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mval: \u001B[0mScanning /home/kristoffel/datasets/dataset-01-Nutrition5k/val... 348 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 348/348 818.2Kit/s 0.0s\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 22/22 33.0it/s 0.7s0.4s\n",
      "                   all      0.856          1\n",
      "Speed: 0.2ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001B[1m/home/kristoffel/models/nutrition5k_yolo11n_cls_val\u001B[0m\n",
      "Ultralytics 8.3.203 üöÄ Python-3.10.12 torch-2.2.0a0+81ea7a4 CUDA:0 (Tesla V100-SXM3-32GB, 32494MiB)\n",
      "\u001B[34m\u001B[1mtrain:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/train... found 2794 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mval:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/val... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mtest:\u001B[0m /home/kristoffel/datasets/dataset-01-Nutrition5k/test... found 348 images in 3 classes ‚úÖ \n",
      "\u001B[34m\u001B[1mtest: \u001B[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 945.9¬±316.1 MB/s, size: 44.2 KB)\n",
      "\u001B[K\u001B[34m\u001B[1mtest: \u001B[0mScanning /home/kristoffel/datasets/dataset-01-Nutrition5k/test... 348 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 348/348 5.7Kit/s 0.1s\n",
      "\u001B[34m\u001B[1mtest: \u001B[0mNew cache created: /home/kristoffel/datasets/dataset-01-Nutrition5k/test.cache\n",
      "\u001B[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 22/22 58.1it/s 0.4s0.1s\n",
      "                   all      0.848          1\n",
      "Speed: 0.2ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001B[1m/home/kristoffel/models/nutrition5k_yolo11n_cls_test\u001B[0m\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f66ba62a",
   "metadata": {},
   "source": [
    "\n",
    "## Notes\n",
    "- If Excel support is missing, run `pip install -U openpyxl` and re-run.  \n",
    "- Expect files like **`dishes.xlsx`** (dish-level totals) and **`dish_ingredients.xlsx`** (ingredient-level).  \n",
    "  The notebook prefers `dishes.xlsx`, and **falls back** to aggregating from `dish_ingredients.xlsx`.  \n",
    "- If your mirror only has `dish_images.pkl`, please re-download to include the metadata sheets (see the Kaggle page).  \n",
    "- For stronger baselines, try `IMGSZ=256/320`, `BATCH=128`, or `yolo11m-cls.pt`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
